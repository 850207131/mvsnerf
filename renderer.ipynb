{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,imageio,lpips\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "\n",
    "from opt import config_parser\n",
    "from data import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# models\n",
    "from models import *\n",
    "from renderer import *\n",
    "from data.ray_utils import get_rays\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# pytorch-lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "\n",
    "\n",
    "from data.ray_utils import ray_marcher\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/anpei/anaconda3/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "def decode_batch(batch):\n",
    "    rays = batch['rays']  # (B, 8)\n",
    "    rgbs = batch['rgbs']  # (B, 3)\n",
    "    return rays, rgbs\n",
    "\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    # to unnormalize image for visualization\n",
    "    # data N V C H W\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "\n",
    "    return (data - mean) / std\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg') \n",
    "mse2psnr = lambda x : -10. * np.log(x) / np.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llff no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['room']):#'fortress','flower','orchids', 'room','leaves','horns','trex','fern'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff \\\n",
    "     --ckpt ./ckpts/mvsnerf-v0.tar \\\n",
    "     --net_type v0 --netwidth 128 --netdepth 6'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4 \n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx\n",
    "            positions = dataset.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset.poses[val_idx[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[1:4]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "\n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "            \n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "    \n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['/mnt/new_disk2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-net-v0-no-color-skip/ckpts/139999.tar']\n",
      "Reloading from /mnt/new_disk2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-net-v0-no-color-skip/ckpts/139999.tar\n",
      "============> rendering dataset <===================\n",
      "41 41 /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/room\n",
      "===> valing index: [35 15 38 21]\n",
      "====> ref idx: [14 39 34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:49<02:29, 50.00s/it]\n",
      "100%|██████████| 4/4 [00:53<00:00, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: room mean psnr 23.934366149970703 ssim: 0.9214743734999353 lpips: 0.2191867232322693\n",
      "=====> all mean psnr 23.934366149970703 ssim: 0.9214743734999353 lpips: 0.2191867232322693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['room']):#'flower','orchids', 'room','leaves','fern','horns','trex','fortress'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/{scene}  \\\n",
    "     --dataset_name llff \\\n",
    "     --ckpt ./ckpts/mvsnerf-v0.tar  \\\n",
    "     --net_type v0 --netwidth 128 --netdepth 6'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # center crop 0.8 ratio\n",
    "            H_crop, W_crop = np.array(rgb_rays.shape[:2])//10\n",
    "            img = img[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            rgb_rays = rgb_rays[H_crop:-H_crop,W_crop:-W_crop]\n",
    "            \n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "        \n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "        \n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nerf no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['/mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-attention-128-sofmax-angle/ckpts//latest.tar']\n",
      "Reloading from /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-attention-128-sofmax-angle/ckpts//latest.tar\n",
      "Not ndc!\n",
      "============> rendering dataset <===================\n",
      "100 [6, 43, 33, 13, 17, 19, 20, 25, 30, 37, 46, 48, 49, 55, 59, 65]\n",
      "===> training index: [6, 43, 33, 13, 17, 19, 20, 25, 30, 37, 46, 48, 49, 55, 59, 65]\n",
      "100 [28, 63, 70, 18]\n",
      "===> valing index: [28, 63, 70, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:12<00:00, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: lego mean psnr 25.38069234029279 ssim: 0.9428600022362317 lpips: 0.1353445127606392\n",
      "=====> all mean psnr 25.38069234029279 ssim: 0.9428600022362317 lpips: 0.1353445127606392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['lego']):#'ship','mic','chair','lego','drums','ficus','materials','hotdog'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 0\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                intrinsic_ref[:2] *= args.imgScale_test/args.imgScale_train\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            img_vis = np.concatenate((torch.cat(torch.split(imgs_source*255, [1,1,1], dim=1),-1).squeeze().permute(1,2,0).cpu().numpy(),img_vis),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['/mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-attention-128-sofmax-angle/ckpts//latest.tar']\n",
      "Reloading from /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-attention-128-sofmax-angle/ckpts//latest.tar\n",
      "Not ndc!\n",
      "============> rendering dataset <===================\n",
      "100 [28, 63, 70, 18]\n",
      "===> valing index: [28, 63, 70, 18]\n",
      "====> ref idx: [6, 43, 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:08<00:00, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: lego mean psnr 17.98688916232244 ssim: 0.8662780820428587 lpips: 0.26200321689248085\n",
      "=====> all mean psnr 17.98688916232244 ssim: 0.8662780820428587 lpips: 0.26200321689248085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate(['lego']):#'ship','mic','chair','lego','drums','ficus','materials','hotdog'\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}  \\\n",
    "     --dataset_name blender --white_bkgd \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = 8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 16\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays-img)**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_depth_spiral.mp4', np.stack(depths_vis), fps=10, quality=10)\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU no fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with nearest 3 views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['./ckpts//mvsnerf-v0.tar']\n",
      "Reloading from ./ckpts//mvsnerf-v0.tar\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 1.0\n",
      "===> training index: [25, 21, 33, 22, 14, 15, 26, 30, 31, 35, 34, 43, 46, 29, 16, 36]\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> scene: 1 mean psnr 26.860625765567647 ssim: 0.936532216389585 lpips: 0.1556858941912651\n",
      "=====> all mean psnr 26.860625765567647 ssim: 0.936532216389585 lpips: 0.1556858941912651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1]):#,8,21,103,114\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "     --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim =  8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'train'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset_train = dataset_dict[args.dataset_name](args, split='train')\n",
    "    dataset_val = dataset_dict[args.dataset_name](args, split='val')\n",
    "    val_idx = dataset_val.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset_val)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            # find nearest image idx from training views\n",
    "            positions = dataset_train.poses[:,:3,3]\n",
    "            dis = np.sum(np.abs(positions - dataset_val.poses[[i],:3,3]), axis=-1)\n",
    "            pair_idx = np.argsort(dis)[:3]\n",
    "            pair_idx = [dataset_train.img_idx[item] for item in pair_idx]\n",
    "            \n",
    "            imgs_source, proj_mats, near_far_source, pose_source = dataset_train.read_source_views(pair_idx=pair_idx,device=device)\n",
    "            volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "            imgs_source = unpreprocess(imgs_source)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering novel views with fixed 3 source views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['/mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-iccv-no-travel_all/ckpts//latest.tar']\n",
      "Reloading from /mnt/new_disk_2/anpei/code/MVS-NeRF/runs_new/mvs-nerf-color-fusion-iccv-no-travel_all/ckpts//latest.tar\n",
      "Not ndc!\n",
      "============> rendering dataset <===================\n",
      "==> image down scale: 1.0\n",
      "===> valing index: [32, 24, 23, 44]\n",
      "====> ref idx: [25, 21, 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 1/4 [00:11<00:35, 11.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 2/4 [00:23<00:23, 11.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:35<00:11, 11.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c148d46489d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_rays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;31m# image should be RGB, IMPORTANT: normalized to [-1,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mimg_gt_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mLPIPS_vgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mloss_fn_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_gt_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lpips/lpips.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mfeats0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlpips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdiffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeats0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfeats1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlpips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "psnr_all,ssim_all,LPIPS_vgg_all = [],[],[]\n",
    "for i_scene, scene in enumerate([1]):#,8,21,103,114\n",
    "    psnr,ssim,LPIPS_vgg = [],[],[]\n",
    "    cmd = f'--datadir /mnt/data/new_disk/sungx/data/mvs_dataset/DTU/mvs_training/dtu/scan{scene}  \\\n",
    "    --dataset_name dtu_ft  \\\n",
    "    --ckpt ./ckpts//mvsnerf-v0.tar'\n",
    "\n",
    "    args = config_parser(cmd.split())\n",
    "    args.use_viewdirs = True\n",
    "\n",
    "    args.N_samples = 128\n",
    "    args.feat_dim = else 8+3*4\n",
    "\n",
    "    # create models\n",
    "    if 0==i_scene:\n",
    "        render_kwargs_train, render_kwargs_test, start, grad_vars = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "        filter_keys(render_kwargs_train)\n",
    "\n",
    "        MVSNet = render_kwargs_train['network_mvs']\n",
    "        render_kwargs_train.pop('network_mvs')\n",
    "\n",
    "\n",
    "    datadir = args.datadir\n",
    "    datatype = 'val'\n",
    "    pad = 24\n",
    "    args.chunk = 5120\n",
    "\n",
    "\n",
    "    print('============> rendering dataset <===================')\n",
    "    dataset = dataset_dict[args.dataset_name](args, split=datatype)\n",
    "    val_idx = dataset.img_idx\n",
    "    \n",
    "    save_as_image = True\n",
    "    save_dir = f'results/test3'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    MVSNet.train()\n",
    "    MVSNet = MVSNet.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        imgs_source, proj_mats, near_far_source, pose_source = dataset.read_source_views(device=device)\n",
    "        volume_feature, _, _ = MVSNet(imgs_source, proj_mats, near_far_source, pad=pad)\n",
    "        imgs_source = unpreprocess(imgs_source)\n",
    "\n",
    "        try:\n",
    "            tqdm._instances.clear() \n",
    "        except Exception:     \n",
    "            pass\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(dataset)):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            rays, img = decode_batch(batch)\n",
    "            rays = rays.squeeze().to(device)  # (H*W, 3)\n",
    "            img = img.squeeze().cpu().numpy()  # (H, W, 3)\n",
    "            depth = batch['depth'].squeeze().numpy()  # (H, W)\n",
    "        \n",
    "            N_rays_all = rays.shape[0]\n",
    "            rgb_rays, depth_rays_preds = [],[]\n",
    "            for chunk_idx in range(N_rays_all//args.chunk + int(N_rays_all%args.chunk>0)):\n",
    "\n",
    "                xyz_coarse_sampled, rays_o, rays_d, z_vals = ray_marcher(rays[chunk_idx*args.chunk:(chunk_idx+1)*args.chunk],\n",
    "                                                    N_samples=args.N_samples)\n",
    "\n",
    "                # Converting world coordinate to ndc coordinate\n",
    "                H, W = img.shape[:2]\n",
    "                inv_scale = torch.tensor([W - 1, H - 1]).to(device)\n",
    "                w2c_ref, intrinsic_ref = pose_source['w2cs'][0], pose_source['intrinsics'][0].clone()\n",
    "                xyz_NDC = get_ndc_coordinate(w2c_ref, intrinsic_ref, xyz_coarse_sampled, inv_scale,\n",
    "                                             near=near_far_source[0], far=near_far_source[1], pad=pad*args.imgScale_test)\n",
    "\n",
    "\n",
    "                # rendering\n",
    "                rgb, disp, acc, depth_pred, alpha, extras = rendering(args, pose_source, xyz_coarse_sampled,\n",
    "                                                                       xyz_NDC, z_vals, rays_o, rays_d,\n",
    "                                                                       volume_feature,imgs_source, **render_kwargs_train)\n",
    "    \n",
    "                \n",
    "                rgb, depth_pred = torch.clamp(rgb.cpu(),0,1.0).numpy(), depth_pred.cpu().numpy()\n",
    "                rgb_rays.append(rgb)\n",
    "                depth_rays_preds.append(depth_pred)\n",
    "\n",
    "            \n",
    "            depth_rays_preds = np.concatenate(depth_rays_preds).reshape(H, W)\n",
    "            depth_rays_preds, _ = visualize_depth_numpy(depth_rays_preds, near_far_source)\n",
    "            \n",
    "            rgb_rays = np.concatenate(rgb_rays).reshape(H, W, 3)\n",
    "            img_vis = np.concatenate((img*255,rgb_rays*255,depth_rays_preds),axis=1)\n",
    "            \n",
    "            if save_as_image:\n",
    "                imageio.imwrite(f'{save_dir}/scan{scene}_{val_idx[i]:03d}.png', img_vis.astype('uint8'))\n",
    "            else:\n",
    "                rgbs.append(img_vis.astype('uint8'))\n",
    "                \n",
    "            # quantity\n",
    "            # mask background since they are outside the far boundle\n",
    "            mask = depth==0\n",
    "            rgb_rays[mask],img[mask] = 0.0,0.0\n",
    "            psnr.append( mse2psnr(np.mean((rgb_rays[~mask]-img[~mask])**2)))\n",
    "            ssim.append( structural_similarity(rgb_rays, img, multichannel=True))\n",
    "            \n",
    "            img_tensor = torch.from_numpy(rgb_rays)[None].permute(0,3,1,2).float()*2-1.0 # image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "            img_gt_tensor = torch.from_numpy(img)[None].permute(0,3,1,2).float()*2-1.0\n",
    "            LPIPS_vgg.append( loss_fn_vgg(img_tensor, img_gt_tensor).item())\n",
    "\n",
    "        print(f'=====> scene: {scene} mean psnr {np.mean(psnr)} ssim: {np.mean(ssim)} lpips: {np.mean(LPIPS_vgg)}')   \n",
    "        psnr_all.append(psnr);ssim_all.append(ssim);LPIPS_vgg_all.append(LPIPS_vgg)\n",
    "\n",
    "    if not save_as_image:\n",
    "        imageio.mimwrite(f'{save_dir}/{scene}_spiral.mp4', np.stack(rgbs), fps=20, quality=10)\n",
    "print(f'=====> all mean psnr {np.mean(psnr_all)} ssim: {np.mean(ssim_all)} lpips: {np.mean(LPIPS_vgg_all)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,torch\n",
    "import sys,os\n",
    "import numpy as np\n",
    "root = '/mnt/new_disk2/anpei/code/MVS-NeRF'\n",
    "os.chdir(root)\n",
    "sys.path.append(root)\n",
    "pairs = torch.load('./configs/pairs.th')\n",
    "\n",
    "# llff\n",
    "root_dir = '/mnt/new_disk_2/anpei/Dataset/MVSNeRF/nerf_llff_data/'\n",
    "for scene in ['horns','leaves', 'room', 'fortress', 'trex', 'orchids','fern']:#\n",
    "    poses_bounds = np.load(os.path.join(root_dir, scene, 'poses_bounds.npy'))  # (N_images, 17)\n",
    "    poses = poses_bounds[:, :15].reshape(-1, 3, 5)  # (N_images, 3, 5)\n",
    "    poses = np.concatenate([poses[..., 1:2], - poses[..., :1], poses[..., 2:4]], -1)\n",
    "\n",
    "    ref_position = np.mean(poses[..., 3],axis=0, keepdims=True)\n",
    "    dist = np.sum(np.abs(poses[..., 3] - ref_position), axis=-1)\n",
    "    pair_idx = np.argsort(dist)[:20]\n",
    "#     pair_idx = torch.randperm(len(poses))[:20].tolist()\n",
    "\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_train'] = np.delete(pair_idx, range(0,20,6))\n",
    "\n",
    "\n",
    "# nerf\n",
    "center_view = {'lego':6,'ship':2,'drums':22,'mic':20,'chair':8,'materials':36,'hotdog':61,'ficus':8}\n",
    "blender2opencv = np.array([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "for i, scene in enumerate(['ship','drums','mic','chair','materials','lego','hotdog','ficus']):\n",
    "    with open(f'/mnt/new_disk_2/anpei/Dataset/nerf_synthetic/{scene}/transforms_train.json', 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    poses = []\n",
    "    ref_idx = torch.randint(0,len(meta['frames']),(1,))\n",
    "    ref_idx = center_view[f'{scene}']\n",
    "    for frame in meta['frames']:\n",
    "        pose = np.array(frame['transform_matrix']) @ blender2opencv\n",
    "        poses += [pose]\n",
    "    poses = np.stack(poses)\n",
    "    \n",
    "    # find nearest image idx\n",
    "    viewing_dir = poses[:,:3,2]\n",
    "    dis = np.sum(viewing_dir * poses[[ref_idx],:3,2], axis=-1)\n",
    "    pair_idx = np.argsort(dis)[::-1][:20]\n",
    "    \n",
    "    pairs[f'{scene}_train'] = list(set(pair_idx) - set(pair_idx[::6]))\n",
    "    pairs[f'{scene}_test'] = pair_idx[::6]\n",
    "    pairs[f'{scene}_val'] = pair_idx[::6]\n",
    "\n",
    "\n",
    "\n",
    "# dtu\n",
    "#      0-4\n",
    "#    10 - 5 \n",
    "#   11  -  18\n",
    "#  27  xx   19\n",
    "# 28    x    38\n",
    "#48     -     39\n",
    "# pairs[f'dtu_train'] = [25,21,33,22,14,15,26,30,31,35,34,43,46,29,16,36]\n",
    "# pairs[f'dtu_val'] = [32,24,23,44]\n",
    "# pairs[f'dtu_test'] = [32,24,23,44]\n",
    "\n",
    "torch.save(pairs,'/mnt/new_disk_2/anpei/code/MVS-NeRF/configs/pairs.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
